---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

# SCRAPPING YOUTUBE 

##Libraries used 
 
```{r}
# install.packages("httr2")
# install.packages("httr")
# install.packages("jsonlite")
# install.packages("tibble")
# install.packages("tidyr")
# install.packages("dotenv")

library(httr2)
library(httr)
library(jsonlite)
library(tibble)
library(tidyr)
library(dotenv)
```

```{r}
load_dot_env(file = ".env") #this function is to load environment variables from a dotenv file, where the file is named .env
API_KEY <- Sys.getenv("API_KEY") #we retrieve the value associated with the environment variable called "API-KEY"
```

```{r}
channel_username <- "CNN" #we define a variable called "CNN" since this is the channel_username that we want to retrive information from. 

#created the API request URL
url_channel <- paste0('https://www.googleapis.com/youtube/v3/channels?part=contentDetails&forUsername=', channel_username, '&key=', API_KEY)
#make the API request URL 
response_channel <- GET(url_channel)
#Parsing API response, we use "text" to specify that we want this type of content to be extracted from the HTTP response.
channel_info <- fromJSON(content(response_channel, "text"))
str(channel_info)



#We see if on the "items" element there is the comment section  
content_details <- channel_info$items$contentDetails
# We can see a data frame containing information about related playlists.
related_playlists <- content_details$relatedPlaylists
str(related_playlists)
```

## Get the video IDs

```{r}
# Function to get video IDs from the "uploads" playlist
get_video_ids <- function() {
  url_channel <- paste0('https://www.googleapis.com/youtube/v3/channels?part=contentDetails&forUsername=', channel_username, '&key=', API_KEY)
  response_channel <- GET(url_channel)
  channel_info <- fromJSON(content(response_channel, "text"))

  uploads_playlist_id <- channel_info$items$contentDetails$relatedPlaylists$uploads

  url_videos <- paste0('https://www.googleapis.com/youtube/v3/playlistItems?part=contentDetails&playlistId=', uploads_playlist_id, '&maxResults=100&key=', API_KEY)

  response_videos <- GET(url_videos)
  videos_info <- fromJSON(content(response_videos, "text"))

  # Extract the video IDs
  video_ids <- videos_info$items$contentDetails$videoId

  return(video_ids)
}

# Retrieve video IDs from the "uploads" playlist
video_ids <- get_video_ids()
```

```{r}
library(tibble)
library(httr)
library(jsonlite)

get_comments_for_video <- function(video_id) {
  max_results <- 100
  keyword <- "Trump"
  next_page_token <- NULL
  all_comments <- character(0)

  repeat {
    url_comments <- paste0('https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=', video_id, '&maxResults=', max_results, '&key=', API_KEY)

    if (!is.null(next_page_token)) {
      url_comments <- paste0(url_comments, '&pageToken=', next_page_token)
    }

    response_comments <- GET(url_comments)
    comments_info <- fromJSON(content(response_comments, "text"))

    current_comments <- comments_info$items$snippet$topLevelComment$snippet$textDisplay

    filtered_comments <- grep(keyword, current_comments, ignore.case = TRUE, value = TRUE)

    all_comments <- c(all_comments, filtered_comments)

    if (is.null(comments_info$nextPageToken)) {
      break
    } else {
      next_page_token <- comments_info$nextPageToken
    }
  }

  # Return the list of filtered comments
  return(all_comments)
}

#Retrieve and filter comments for each video in the playlist
all_video_comments <- list()

for (video_id in video_ids) {
  comments_for_video <- get_comments_for_video(video_id)
  all_video_comments[[video_id]] <- comments_for_video
}

# Create a single tibble for all comments
comments_tibble <- tibble(comments = unlist(all_video_comments))

# Now, 'comments_tibble' contains a tibble with a column named 'comments'
print(comments_tibble)

```

### Text_mining

```{r}
library(tidytext)
library(dplyr)
```

```{r}
get_sentiments("nrc")
```

```{r}
# positive
nrc_positive <- get_sentiments("nrc") %>% 
  filter(sentiment == "positive")

# negative
nrc_negative <- get_sentiments("nrc") |> 
  filter(sentiment == "negative")

# anger
nrc_anger <- get_sentiments("nrc") |> 
  filter(sentiment == "anger")

# trust
nrc_trust <- get_sentiments("nrc") |> 
  filter(sentiment == "trust")
```

```{r}
stop_words
```

```{r}
#unnesting the words
yt_word <- comments_tibble |> 
  unnest_tokens(word, comments) |> 
  anti_join(stop_words) 
```

## Sentiment Analysis 

```{r}
# trump_positive
yt_trump_positive <- yt_word |> 
    inner_join(nrc_positive) |> 
    count(word, sort = TRUE)

yt_trump_positive$type <- c("positive")
print(yt_trump_positive)

# trump_negative
yt_trump_negative <- yt_word |> 
  inner_join(nrc_negative) |> 
  count(word, sort = TRUE)

yt_trump_negative$type <- c("negative")
print(yt_trump_negative)

# trump_anger
yt_trump_anger <- yt_word |> 
  inner_join(nrc_anger) |> 
  count(word, sort = TRUE)

yt_trump_anger$type <- c("anger")
print(yt_trump_anger)

# trump_trust
yt_trump_trust <- yt_word |> 
  inner_join(nrc_trust) |> 
  count(word, sort = TRUE)
yt_trump_trust$type = c("trust")
print(yt_trump_trust)
```

Merge all words with count

```{r}
yt_trump <- rbind(yt_trump_positive, yt_trump_negative,yt_trump_anger, yt_trump_trust)
```

## Visualize the result
```{r}
library(ggplot2)

yt_trump_plot <- yt_trump |> 
  group_by(type) |> 
  top_n(8, wt = n) |>  #selected the top 8 more repeated words for each sentiment 
  ggplot(aes(word, n, fill = type)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~type, nrow = 3, scales = "free_x") +
  theme(axis.text.x = element_text(size = 5.5))


print(yt_trump_plot)
```


## Term frequency 
```{r}
total_words <- yt_trump |> 
  group_by(type) |> 
  summarize(total = sum(n))

total_words
```

```{r}
words <- yt_trump |> 
  left_join(total_words, by = "type")
  
words <- words |> 
  mutate(frequency = n / total)

words
```

```{r}
library(ggplot2)

#we calculate the distribution and put it in the x axis, filling by type
ggplot(words, aes(frequency)) +
  #we create the bars histogram
  geom_histogram(show.legend = TRUE) +
  #we set the limit for the term frequency in the x axis
  xlim(NA, 0.015)
```

```{r}
ggplot(words, aes(frequency, fill = type)) +
  #we create the bars histogram
  geom_histogram(show.legend = TRUE) +
  #we set the limit for the term frequency in the x axis
  xlim(NA, 0.015) +
  #plot settings
  facet_wrap(~type, ncol = 2, scales = "free_y")
```

```{r}
freq_rank <- words %>% 
  group_by(type) %>% 
  #we create the column for the rank with row_number by type
  mutate(rank = row_number()) %>%
  ungroup()

freq_rank

freq_rank |> group_by(type) |> filter(rank == 1 | rank == 2)
```

```{r}
freq_rank %>% 
  ggplot(aes(rank, frequency, color = type)) + 
  #plot settings
  geom_line(linewidth = 1.1, alpha = 0.8, show.legend = TRUE) +
  theme_minimal()
```

Sentiment plot of all comments on YouTube CNN channel 

```{r}
sentiments <- yt_trump %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  rename(count = n)
sentiments
```

```{r}
library(ggplot2)

sentiments %>%
  #we weight by the count column: a term and its sentiment associated multiplied by count
  count(sentiment, word, wt = count) %>%
  ungroup() %>%
  #we filter by words appearing more than 90 times in the comments
  filter(n >= 90) %>%
  #create a new column called n that is equal to the count, but with the sign flipped
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  #reorder for descending order of bars
  mutate(term = reorder(word, n)) %>%
  #plot settings
  ggplot(aes(n, term, fill = sentiment)) +
  geom_col() +
  labs(x = "Number of mentions (contribution to sentiment)", y = NULL)
```

As visualization above, it becomes evident that the lexicon associated with Trump leans more towards negative sentiments than positive ones. The prevalence of words expressing unfavorable sentiments surpasses those conveying a positive tone in discussions related to Trump on the YouTube comments for the CNN channel and playlist about "Politics". 

# SCRAPPING REDDIT

### Authentification

### Libraries that we have used

```{r}
library(httr)
library(httr2)
library(jsonlite)
library(dplyr)
library(scrapex)
library(tidyr)
```

### OAuth 2.0

```{r}
#install.packages("dotenv")
library(dotenv)
dotenv::load_dot_env(file = ".env")
access_token = Sys.getenv("access_token")
```

### Requesting the data using API

```{r}
search_url <- "https://api.reddit.com/r/politics/comments"

search_params <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "best",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

# best
req <- GET(
  url = search_url,
  query = search_params
)

print(req)

status_code(req)

# Parse the JSON content from the response
response_json <- content(req, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list <- fromJSON(response_json)

# Extract comments from the search results
link_comments <- data.frame(response_list$data$children$data$body)
print(link_comments)

link_comments <- link_comments %>%
  rename("text" = "response_list.data.children.data.body")

```

```{R}
# relevance
search_params_2 <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "relevance",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

req2 <- GET(
  url = search_url,
  query = search_params_2
)

print(req2)

# Parse the JSON content from the response
response_json_2 <- content(req2, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list_2 <- fromJSON(response_json_2)

# Extract comments from the search results
link_comments_2 <- data.frame(response_list_2$data$children$data$body)
print(link_comments_2)

link_comments_2 <- link_comments_2 %>%
  rename("text" = "response_list_2.data.children.data.body")

```

```{R}
# top
search_params_3 <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "top",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

req3 <- GET(
  url = search_url,
  query = search_params_3
)

print(req3)

# Parse the JSON content from the response
response_json_3 <- content(req3, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list_3 <- fromJSON(response_json_3)

# Extract comments from the search results
link_comments_3 <- data.frame(response_list_3$data$children$data$body)
print(link_comments_3)

link_comments_3 <- link_comments_3 %>%
  rename("text" = "response_list_3.data.children.data.body")
```

```{r}
# hot
search_params_4 <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "hot",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

req4 <- GET(
  url = search_url,
  query = search_params_4
)

print(req4)

# Parse the JSON content from the response
response_json_4 <- content(req4, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list_4 <- fromJSON(response_json_4)

# Extract comments from the search results
link_comments_4 <- data.frame(response_list_4$data$children$data$body)
print(link_comments_4)

link_comments_4 <- link_comments_4 %>%
  rename("text" = "response_list_4.data.children.data.body")
```

```{r}
# new
search_params_5 <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "new",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

req5 <- GET(
  url = search_url,
  query = search_params_5
)

print(req5)

# Parse the JSON content from the response
response_json_5 <- content(req5, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list_5 <- fromJSON(response_json_5)

# Extract comments from the search results
link_comments_5 <- data.frame(response_list_5$data$children$data$body)
print(link_comments_5)

link_comments_5 <- link_comments_5 %>%
  rename("text" = "response_list_5.data.children.data.body")
```


```{R}
trump = rbind(link_comments, link_comments_2, link_comments_3, link_comments_4, link_comments_5)

print(trump)
```

### Text_mining

```{r}
library(tidytext)
nrc = get_sentiments("nrc")
```

### sentimental analysis
### nrc analysis (word count)

````{R}
# Using nrc first to segment the sentiments
# positive 
nrc_positive <- get_sentiments("nrc") %>% 
  filter(sentiment == "positive")

# negative
nrc_negative <- get_sentiments("nrc") |> 
  filter(sentiment == "negative")

# anger
nrc_anger <- get_sentiments("nrc") |> 
  filter(sentiment == "anger")

# trust
nrc_trust <- get_sentiments("nrc") |> 
  filter(sentiment == "trust")
```

```{R}
# unnesting the word
word = trump |> unnest_tokens(word, text)
print(word)
```

```{R}
# trump_joy
trump_positive <- word |> 
    #we combine both lists, NRC and comments words
    inner_join(nrc_positive) %>%
    #we count the mentions of each word to find the most frequent
    count(word, sort = TRUE)

trump_positive$type <- c("positive")
print(trump_positive)

# trump_negative
trump_negative <- word |> 
  inner_join(nrc_negative) |> 
  count(word, sort = TRUE)

trump_negative$type <- c("negative")
print(trump_negative)

# trump_anger
trump_anger <- word |> 
  inner_join(nrc_anger) |> 
  count(word, sort = TRUE)

trump_anger$type <- c("anger")
print(trump_anger)

# trump_trust
trump_trust <- word |> 
  inner_join(nrc_trust) |> 
  count(word, sort = TRUE)

trump_trust$type = c("trust")
print(trump_trust)

trump_nrc = rbind(trump_positive, trump_negative, trump_anger, trump_trust)
print(trump_nrc)

# erase the word "vote" since it is neutral and duplicated a lot
trump_nrc = trump_nrc |> filter(word != "vote")
print(trump_nrc)
```

### graph for nrc analysis

```{r}
library(ggplot2)

sentiment_nrc <- trump_nrc |> filter(n > 11) |> 
ggplot(aes(word, n, fill = type)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~type, nrow = 4, scales = "free_x") +
  ylab(NULL) + 
  theme_minimal() +
  theme(strip.text = element_text(face = "bold"),
        axis.text.x = element_text(size = 5.8, face = 'bold'),
        plot.margin = unit(c(1, 1, 1, 0.1), "cm"),
        legend.position = "none")


print(sentiment_nrc)
```

### Term frequency analysis

```{r}
total_comment_words <- trump_nrc %>% 
  #we group by types to sum all the totals in the n column of comment_words
  group_by(type) %>% 
  #we create a column called total with the total of words by type
  summarize(total = sum(n))

total_comment_words
```

```{r}
comment_words <- left_join(trump_nrc, total_comment_words)
comment_words
```

```{r}
comment_words <- comment_words %>%
  #we add a column for term_frequency in each type
  mutate(term_frequency = n/total)

comment_words
```

```{r}
library(ggplot2)

#we calculate the distribution and put it in the x axis, filling by type
ggplot(comment_words, aes(term_frequency)) +
  #we create the bars histogram
  geom_histogram(show.legend = TRUE) +
  #we set the limit for the term frequency in the x axis
  xlim(NA, 0.03)
```

```{r}
ggplot(comment_words, aes(term_frequency, fill = type)) +
  #we create the bars histogram
  geom_histogram(show.legend = TRUE) +
  #we set the limit for the term frequency in the x axis
  xlim(NA, 0.04) +
  #plot settings
  facet_wrap(~type, ncol = 2, scales = "free_y")
```

```{r}
freq_by_rank <- comment_words %>% 
  group_by(type) %>% 
  #we create the column for the rank with row_number by type
  mutate(rank = row_number()) %>%
  ungroup()

freq_by_rank

freq_by_rank |> group_by(type) |> filter(rank == 1 | rank == 2)
```

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, term_frequency, color = type)) + 
  #plot settings
  geom_line(linewidth = 1.1, alpha = 0.8, show.legend = TRUE) +
  theme_minimal()
```

# COMPARING YOUTUBE AND REDDIT COMMENTS RELATED TO TRUMP 
### Youtube
```{r}
words_yt <- words |> # YouTube
  mutate(platform = c("YouTube"))

words_yt <- as.data.frame(words_yt) 
words_yt <- words_yt |> rename("term_frequency" = "frequency")
words_yt <- words_yt |> filter(word != "vote")
```

### Reddit
```{r}
comment_words_rd <- comment_words |> # Reddit 
  mutate(platform = c("Reddit"))

comment_words_rd <- comment_words_rd |> filter(word != "influence")

combined_yt_rd <- rbind(words_yt, comment_words_rd)
combined_yt_rd |> group_by(platform) |>  arrange(desc(term_frequency))

```

### Visualization
```{r}
library(ggplot2)

combined_yt_rd |> 
  filter(type == "positive" | type == "negative") |> 
  filter(term_frequency > 0.016) |> 
  #reorder for descending order of bars
  mutate(term = reorder(word, term_frequency)) %>%
  #plot settings
  ggplot(aes(term_frequency, word, fill = type)) +
  geom_col() +
  labs(x = "Term frequencies (contribution to sentiment)", y = NULL) +
  facet_wrap(~platform, ncol = 2, scales = "free_y") +
  theme_minimal() +
  theme(strip.text = element_text(face = "bold"))
```

### Word cloud 
### Libraries needed for this plot 
```{r}
#install.packages("wordcloud")
library(RColorBrewer)
library(wordcloud)
library(reshape2)
library(wordcloud2)
```

### WordCloud2 - Youtube
```{r}
yt_wordclod <- combined_yt_rd |>
  filter(platform == "YouTube") |> 
wordcloud2(color = "random-dark",
            fontWeight = 550,
            size = 2,
            widgetsize = c(900, 500))

yt_wordclod
```

### WordCloud2 - Reddit
```{r}
Rd_wordcloud <- combined_yt_rd |>
  filter(platform == "Reddit") |> 
wordcloud2(color = "random-dark",
            fontWeight = 550,
            size = 0.8,
            widgetsize = c(900, 500))

Rd_wordcloud
```

### Shiny App
```{r}

```


